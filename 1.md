---
layout: post
mathjax: true
title: Lecture 1 Optimization
---

## Topics
- Likelihoods
- Maximum likelihood estimation
- Expectation of a random variable
- Bias of a random variable
- variance of a random variable

## Distribution
Observations $$X_1, ..., X_n$$ assumed to come from some density function. 

If the density function is known then there is no statistical problem. 

If knowing the density function solves any problem then we want to estimate it from the data.

### Example 1. 10 observations
X: 2.54, 3.41, 3.88, 3.69, 3.51, 3.27. 3,43, 4.16, 3.38, 2.49

We assume there have come from a normal distribution, but do not know which one.

\begin{equation}

f(x | \mu, \sigma^2) = \frac{1}{\sqrt{2 \pi \sigma}} e^{-\frac{1}{2} (x - \mu) ^ 2 / \sigma ^ 2}

\end{equation}

The idea is to use the 10 observations to estimate $$ \mu $$ and $$ \sigma $$.

The usual way to connect up the data and the density is to plug the data into the density. The end result is the likelihood function.

\begin{equation}
l(\mu, \sigma^2) = \prod\limits_{i=1}^{10} f(x_i | \mu, \sigma^2)
\end{equation}

The next step is to find the estimate of $$\mu$$ and $$\sigma$$ by maximizing the likelihood function.

For the normal distribution, that solution is the mean and variance.

\begin{equation}
\begin{split}
\hat{\mu} = \bar{X} = \frac{1}{n} \sum\limits_{i=1}^{10} x_i = 3.38 \\
\hat{\sigma^2} = \frac{1}{n} \sum\limits_{i=1}^{10} (x_i - \bar{x}) ^ 2  \mathtt{(population variance)}
\end{split}
\end{equation}

but most people use the sample variance

\begin{equation}
\frac{1}{n-1} \sum\limits_{i=1}^{10} (x_i - \bar{x}) = 0.28
\end{equation}

### Example 2
Suppose $$ x_1, ..., x_n$$ are from a Poisson distribution.

The density function is 

\begin{equation}
P(x | \theta) = \frac{\theta ^ x}{x!} e^{-\theta}
\end{equation}
for $$ x= 0, 1, ..., \theta > 0$$

The likelihood functions is

\begin{equation}
l(\theta) = \prod\limits_{i=1}^{n} \theta^{x_i} e^{-\theta}
\end{equation}

This is actually a pseudo-likelihood because we left off $$ \frac{1}{x!}$$.

However, they both have the same maximum so the value of $$ \theta $$ that maximizes $$ l(\theta) $$ would also maximize the true likelihood.

More precisely:

If $$ l(\theta) > l(\theta')$$ then $$ \frac{l(\theta)}{x!} > \frac{l(\theta')}{x!}$$.

So $$ \operatorname_{\theta} l(\theta) = \operatorname_{\theta} \frac{l(\theta))}{x!}$$.

Often when working with distributions it is easier to work with these "unnormalized" functions where dividing the function by some number that does not include the parameters yields the true value of the distirbution.

Rather than telling you the solution to $$ \hat{\theta} $$, let's derive it this time.

We can rewrite the above as

\begin{equation}
l(\theta) = \theta^{n \bar{x}} e^{-n \theta}
\end{equation}

Thus we call $$ \bar{x} $$ a *sufficient statistics* because it's sufficient to give you a likelihood function without needing the individual data points.

To maximize the likelihood, we first make another transformation

\begin{equation}
\operatorname{argmax}_\theta l(\theta) = \operatorname{argmax}_\log(\theta)
\end{equation}

This converts all the ugly multiplicative terms in $$ l(\theta) $$ into simple additive terms

\begin{equation}
\log(l(\theta)) = n\bar{x} - n \theta
\end{equation}

Now recall your calculus classes. A minumum of a function can be found by taking the derivative of the function, setting it equal to zero, and solving for the parameter(s).

Since we want to *maximize* the likelihood of the data, we can minimize the negative of the log-likelihood.

\begin{equation}
\operatorname{argmin}_\theta - log(l(\theta)) = \operatorname{argmin}_\theta - n\bar{x} \log(\theta) + n\theta
\end{equation}

Taking the derivative of the above with respect to our parameter $$ \theta $$:

\begin{equation}
\Delta_\theta = -n\bar{x}(\frac{1}{\theta}) + n
\end{equation}

Setting this equal to zero and solving:

\begin{eqaution}
\begin{split}
-n \bar{x} (\frac{1}{\theta}) + n = 0\\
-n\bar{x}(\frac{1}{\theta}) = -n\\
\bar{x}\frac{1}{\theta} = 1
\frac{1}{\theta} = \frac{1}{\bar{x}} \\
\hat{\theta} = \bar{x}
\end{split}
\end{equation}

We often call $$ \hat{theta} $$ an **estimator** of $$ \theta $$. It is one value derived according to a certain principle: maximum likelihood estimation (MLE).

But there are other ways to estimate $$ \theta $$.
For example, we could have chosen $$ \hat{\theta} = \operatorname{median}(x_1, ..., x_n) $$.

In statistics, we typically compare estimators based on two properties:
1. Bias: $$ \mathcal{E}[\hat{\theta} - \theta] $$ (how much will $$ \hat{\theta} $$ deviate from the truth) 
2. Variance: $$ \mathcal{E} [(\hat{\theta} - \mathcal{E}(\hat{theta})) ^ 2]$$ (How much will $$ \hat{\theta} $$ vary around its mean.)

IdeallyL Small or no bias and low variance.

What is biase and variacne of $$ \hat{\theat} = \bar{x}$$ ?
Before the data, $$ \bar{x} = \frac{x_1 + x_2 + ... + x_n}{n} $$ is unknown. That is, it is a random variable and has an expected value.

\begin{equation}
\mathcal{E}[\bar{x}] = \mathcal{E}[\frac{x_1 + x_2 + ... + x_n}{n}] = \frac{1}{n} \mathcal{E}[x_1 + x_2 + ... + x_n] = \frac{1}{n}(\mathcal{E}(x_1) + \mathcal{E}(x_2) + ... + \mathcal{E}(x_n))
\end{equation}

where the laste step is due to the linearlty of expectation:

\begin{equation}
\mathcal{E}[a + b] = \mathcal{E}[a] + \mathcal{E}[b]
\end{equation}

\begin{equation}
\frac{1}{n}(\mathcal{E}[x_1] + \mathcal{E}[x_2] + ... + \mathcal{E}[x_n]) = \mathcal{E}[x]
\end{equation}

For the Poisson, $$ \mathcal{E}[x] = \theta $$.

The bias of an estimator is how much its expectation deviates from the value of the true parameter.

Since $$ \hat{\theta} = \bar{x}$$ and $$ \mathcal{}$$
