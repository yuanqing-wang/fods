---
layout: post
mathjax: true
title: Lecture 1 Optimization
---

## Topics
- Likelihoods
- Maximum likelihood estimation
- Expectation of a random variable
- Bias of a random variable
- variance of a random variable

## Distribution
Observations $$X_1, ..., X_n$$ assumed to come from some density function. 

If the density function is known then there is no statistical problem. 

If knowing the density function solves any problem then we want to estimate it from the data.

### Example 1. 10 observations
X: 2.54, 3.41, 3.88, 3.69, 3.51, 3.27. 3,43, 4.16, 3.38, 2.49

We assume there have come from a normal distribution, but do not know which one.

\begin{equation}

f(x | \mu, \sigma^2) = \frac{1}{\sqrt{2 \pi \sigma}} e^{-\frac{1}{2} (x - \mu) ^ 2 / \sigma ^ 2}

\end{equation}

The idea is to use the 10 observations to estimate $$ \mu $$ and $$ \sigma $$.

The usual way to connect up the data and the density is to plug the data into the density. The end result is the likelihood function.

\begin{equation}
l(\mu, \sigma^2) = \prod\limits_{i=1}^{10} f(x_i | \mu, \sigma^2)
\end{equation}

The next step is to find the estimate of $$\mu$$ and $$\sigma$$ by maximizing the likelihood function.

For the normal distribution, that solution is the mean and variance.

\begin{equation}
\begin{split}
\hat{\mu} = \bar{X} = \frac{1}{n} \sum\limits_{i=1}^{10} x_i = 3.38 \\
\hat{\sigma^2} = \frac{1}{n} \sum\limits_{i=1}^{10} (x_i - \bar{x}) ^ 2  \mathtt{(population variance)}
\end{split}
\end{equation}

but most people use the sample variance

\begin{equation}
\frac{1}{n-1} \sum\limits_{i=1}^{10} (x_i - \bar{x}) = 0.28
\end{equation}

### Example 2
Suppose $$ x_1, ..., x_n$$ are from a Poisson distribution.

The density function is 

\begin{equation}
P(x | \theta) = \frac{\theta ^ x}{x!} e^{-\theta}
\end{equation}
for $$ x= 0, 1, ..., \theta > 0$$

The likelihood functions is

\begin{equation}
l(\theta) = \prod\limits_{i=1}^{n} \theta^{x_i} e^{-\theta}
\end{equation}

This is actually a pseudo-likelihood because we left off $$ \frac{1}{x!}$$.

However, they both have the same maximum so the value of $$ \theta $$ that maximizes $$ l(\theta) $$ would also maximize the true likelihood.

More precisely:

If $$ l(\theta) > l(\theta')$$ then $$ \frac{l(\theta)}{x!} > \frac{l(\theta')}{x!}$$.

So $$ \operatorname_{\theta} l(\theta) = \operatorname_{\theta} \frac{l(\theta))}{x!}$$.

Often when working with distributions it is easier to work with these "unnormalized" functions where dividing the function by some number that does not include the parameters yields the true value of the distirbution.

Rather than telling you the solution to $$ \hat{\theta} $$, let's derive it this time.

