Foundations of Data Science
===========================

Course Director: Tansey Wesley, Ph.D. 

Teaching Assistant: Yuanqing Wang

## Office Hours and Contact Information
Wesley Tansey tanseyw@mskcc.org

Office hours by appointment; Dr. Tansey makes himself available to students and to course Faculty throughout the course.

## Course Description
The course will give trainees a common foundation upon which they can talk about, reason through, and work with data. We will start with the very basics of data analysis: modeling and statistical inference. On top of these two core ideas we'll build more advanced topics like graphical models, posterior inference, and optimization. Rounding out the topics will be some of the most common methodological applications: mixture models, matrix factorization, and regularized regression. At each step in this series, the focus will be on viewing data science holistically through the lens of probabilistic modeling.

## Schedule
- Optimization
    1. Likelihoods, MLE, expectations, bias, variance
    2. Logistic regression, convexity, gradient descent, newton's method
    3. Priors, penalties, MAP, L1 vs L2

- Baeysian Inference
    4. Posterior distributions, Bayes' rule, uncertainty intervals, Bayes' estimate, posterior predictive
    5. Graphical models, conjugate priors, Gibbs sampling
    6. Non-conjugate models, Metropolis Hastings, Bayes vs. Optimization

- Applications with Bayesian vs optimization comparisons
    7. Latent variable models, mixture models, data augmentation, EM
    8. Matrix factorization, tensors, latent factor modeling, alternating minimization, selecting the number of factors
    9. Missing data, informative missingness, causal inference
    10. Time series and spatial data, Gaussian processes, trend filtering

- Can't we all just get along? Hybrid (optimization + Bayes) approaches
    11. Hamiltonian Monte Carlo
    12. Empirical Bayes
    13. Variational inference
    14. Deep generative models

## Course Objectives
At the completion of this course the learner will become familiar with:
- Modeling and analyzing data along with tradeoffs in different approaches
- Numerical optimization for maximum likelihood and a posteriori estimates
- Sampling approaches for conjugate and non-conjugate posterior densities
- Latent variable models, hierarchical models, graphical models, and spatio-temporal models
- Efficient hybrid optimization approaches to approximate posterior inference

## Course Frequency and Schedule
Offered in the Fall Semester. All classes are Mondays and Wednesdays, 10am-11:30am, for a total of 13 classes. Total contact minutes: 1170.

## Text and Materials
Students will run python. The language is free and runs on recent versions of Linux, Mac OS X, and Microsoft Windows.

While the course does not require the use of a specific textbook, the following resources are recommended:
- Machine Learning: A Probabilistic Perspective, Kevin Murphy
A great in-depth reference text for many probabilistic ML algorithms
- Convex Optimization, Stephen Boyd and Lieven Vandenberghe
The definitive text on optimization.
Available for free online: https://web.stanford.edu/~boyd/cvxbook/
- Bayesian Data Analysis, Gelman et al
	The definitive text on Bayesian modeling.
	Available for free online: http://www.stat.columbia.edu/~gelman/book/
  
  
## Teaching Format
The course is based on lectures.

## Method of Evaluation
Students will be graded on several take-home problem sets

## Grade
Pass/Fail

## Student Learning Outcomes
By the end of the course, students will have acquired the skills to model complex datasets, build bespoke models that fit unique or novel experimental designs, and construct inference algorithms for these models that are both efficient and robust. These skills include:
- Maximum likelihood estimation and optimization (Sessions 1-3)
- Generative modeling and Bayesian statistics (Sessions 3-9)
- Approximate posterior inference (Sessions 5, 6, and 11-13)
- Modeling dependent data including matrices, mixture models, time series, spatial data, and treatment-biased longitudinal data (Sessions 7-10)
- Causal inference (Session 10)
- Probabilistic deep learning (Session 14)

## Academic Integrity
Students are expected to abide by the Weill Cornell Graduate School Code of Academic Integrity.








